{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning (Q-Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e825de35",
   "metadata": {},
   "source": [
    "\n",
    "## What is Reinforcement Learning?\n",
    "\n",
    "Reinforcement learning is like teaching through trial and error - an agent learns by trying actions, receiving feedback (rewards), and gradually improving its behavior. Think of training a pet with treats, learning to ride a bike through practice, or mastering a video game by playing it repeatedly.\n",
    "\n",
    "\n",
    "At its core, RL is about teaching an **Agent** (our algorithm) how to behave in an **Environment** (our game or problem) to maximize a **Reward**.\n",
    "\n",
    "It's based on a simple loop:\n",
    "1.  The **Agent** observes the **State** (S) of the environment.\n",
    "2.  The **Agent** chooses an **Action** (A).\n",
    "3.  The **Environment** reacts: it gives the agent a **Reward** (R) and a new **State** (S').\n",
    "4.  The Agent learns from this (S, A, R, S') tuple and the loop repeats.\n",
    "\n",
    "Our goal is to create an agent that, from any state, learns to pick the action that will give it the most *cumulative future reward*.\n",
    "\n",
    "\n",
    "[![Reinforcement Learning Diagram](./img/RL_loop.png)](https://gymnasium.farama.org/introduction/basic_usage/)\n",
    "\n",
    "\n",
    "## Understanding Q-Learning Intuitively\n",
    "\n",
    "Q-learning builds a giant ‚Äúcheat sheet‚Äù called a Q-table that tells the agent how good each action is in each situation:\n",
    "- Rows = different situations (states) the agent can encounter\n",
    "- Columns = different actions the agent can take\n",
    "- Values = how good that action is in that situation (expected future reward)\n",
    "\n",
    "### The Learning Process\n",
    "\n",
    "1. Try an action and see what happens (reward + new state)\n",
    "2. Update your cheat sheet: ‚ÄúThat action was better/worse than I thought‚Äù\n",
    "3. Gradually improve by trying actions and updating estimates\n",
    "4. Balance exploration vs exploitation: Try new things vs use what you know works\n",
    "\n",
    "Why it works: Over time, good actions get higher Q-values, bad actions get lower Q-values. The agent learns to pick actions with the highest expected rewards.\n",
    "\n",
    "\n",
    "<img src=\"https://huggingface.co/blog/assets/70_deep_rl_q_part1/Q-function-2.jpg\" alt=\"Q function\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment\n",
    "\n",
    "We'll create a simple 4x4 Grid World.\n",
    "\n",
    "* **Agent (A):** Our learner.\n",
    "* **Goal (G):** A good state with a +10 reward.\n",
    "* **Trap (T):** A bad state with a -10 reward.\n",
    "* **States (S):** Any grid position, represented as `(row, col)`.\n",
    "* **Actions (A):** Up, Down, Left, Right.\n",
    "* **Rewards (R):**\n",
    "    * -0.1 for every step (to encourage speed).\n",
    "    * +10 for reaching the Goal.\n",
    "    * -10 for falling in the Trap.\n",
    "\n",
    "Here's our map:\n",
    "```\n",
    "  (0,0) (0,1) (0,2) (0,3)\n",
    "  (1,0) (1,1) (1,2) (1,3)\n",
    "  (2,0) (2,1) [T]   (2,3)\n",
    "  (3,0) (3,1) (3,2) [G]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid World: 4x4\n",
      "Start: (0, 0), Goal: (3, 3), Trap: (2, 2)\n",
      "Actions: [(0, '‚Üë'), (1, '‚Üì'), (2, '‚Üê'), (3, '‚Üí')]\n"
     ]
    }
   ],
   "source": [
    "# Environment parameters\n",
    "GRID_ROWS = 4\n",
    "GRID_COLS = 4\n",
    "START_STATE = (0, 0)\n",
    "GOAL_STATE = (3, 3)\n",
    "TRAP_STATE = (2, 2)\n",
    "\n",
    "# Rewards\n",
    "REWARD_STEP = -0.1\n",
    "REWARD_GOAL = 10\n",
    "REWARD_TRAP = -10\n",
    "\n",
    "# Actions (0: Up, 1: Down, 2: Left, 3: Right)\n",
    "# We use indices for easier lookup in our Q-table\n",
    "ACTIONS = [0, 1, 2, 3]\n",
    "ACTION_NAMES = [\"‚Üë\", \"‚Üì\", \"‚Üê\", \"‚Üí\"]\n",
    "\n",
    "# A dictionary to map action indices to (row_change, col_change)\n",
    "# This is how we'll move on the grid\n",
    "ACTION_VECTORS = {\n",
    "    0: (-1, 0), # Up\n",
    "    1: (1, 0),  # Down\n",
    "    2: (0, -1), # Left\n",
    "    3: (0, 1)   # Right\n",
    "}\n",
    "\n",
    "print(f\"Grid World: {GRID_ROWS}x{GRID_COLS}\")\n",
    "print(f\"Start: {START_STATE}, Goal: {GOAL_STATE}, Trap: {TRAP_STATE}\")\n",
    "print(f\"Actions: {list(zip(ACTIONS, ACTION_NAMES))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Agent's Brain (The Q-Table)\n",
    "\n",
    "How will our agent \"learn\"? It will store its knowledge in a **Q-Table**.\n",
    "\n",
    "* It's a big table (a 3D NumPy array in our case) where:\n",
    "    * The rows represent the grid's **rows**.\n",
    "    * The columns represent the grid's **columns**.\n",
    "    * The 3rd dimension represents the **action**.\n",
    "\n",
    "* `Q_table[row, col, action]` will store a number, the \"Q-value\".\n",
    "* This **Q-value** is the agent's *prediction* of the total future reward it will get if it takes that `action` from that `(row, col)` state.\n",
    "\n",
    "We initialize this table to all zeros, because our agent starts out knowing nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Q-Table (all zeros):\n",
      "[[[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]]\n",
      "Shape of Q-Table: (4, 4, 4)\n"
     ]
    }
   ],
   "source": [
    "Q_table = np.zeros((GRID_ROWS, GRID_COLS, len(ACTIONS)))\n",
    "\n",
    "print(\"Initial Q-Table (all zeros):\")\n",
    "print(Q_table)\n",
    "print(f\"Shape of Q-Table: {Q_table.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Learning Algorithm (Q-Learning)\n",
    "\n",
    "This is the most important concept. How do we update the Q-table?\n",
    "\n",
    "When the agent takes an **action** ($a$) from a **state** ($s$) and moves to a **new state** ($s'$) and gets a **reward** ($r$), we update the table using the **Bellman Equation**:\n",
    "\n",
    "$$Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]$$\n",
    "\n",
    "Let's break this down in plain English:\n",
    "\n",
    "* $Q(s, a) \\leftarrow ...$\n",
    "    * \"Update the Q-value for the old state and action...\"\n",
    "* $... + \\alpha [...]$ \n",
    "    * \"...by adding a small portion (the **learning rate**, $\\alpha$) of new information.\"\n",
    "* $... [r + \\gamma \\max_{a'} Q(s', a') ...]$\n",
    "    * This is the \"new information.\" It's the **reward ($r$) we just got**...\n",
    "    * ...plus the **best possible Q-value we can get from our new state ($s'$)**, (multiplied by a **discount factor**, $\\gamma$, which values immediate rewards over future ones).\n",
    "* $... - Q(s, a)]$\n",
    "    * The full term in the brackets `[ ... ]` is the \"temporal difference error\": the difference between our *new guess* ($r + \\gamma \\max...$) and our *old guess* ($Q(s, a)$).\n",
    "\n",
    "We also need a **strategy** for picking actions. We'll use **Epsilon-Greedy ($\\epsilon$-greedy)**:\n",
    "\n",
    "* With probability `1 - epsilon`: **Exploitation** (pick the best known action from the Q-table).\n",
    "* With probability `epsilon`: **Exploration** (pick a random action to discover new paths).\n",
    "\n",
    "This ensures our agent doesn't just get stuck in the first \"good\" path it finds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters set.\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.1   # Alpha (Œ±): How quickly the agent learns.\n",
    "discount_factor = 0.9 # Gamma (Œ≥): How much the agent values future rewards.\n",
    "epsilon = 1.0         # Initial exploration rate\n",
    "max_epsilon = 1.0     # Maximum exploration\n",
    "min_epsilon = 0.01    # Minimum exploration\n",
    "epsilon_decay = 0.001 # Rate at which exploration decreases\n",
    "\n",
    "# Training parameters\n",
    "total_episodes = 10000 # How many \"games\" to play\n",
    "max_steps = 100        # Max steps per game (to prevent infinite loops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "\n",
    "We need a few functions to run our simulation.\n",
    "\n",
    "1.  `choose_action(state)`: Implements the $\\epsilon$-greedy strategy.\n",
    "2.  `take_action(state, action_index)`: Simulates taking an action and returns the `(new_state, reward, done)` tuple. This function will also handle \"bumping into walls\" (staying in the same state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "def choose_action(state, current_epsilon):\n",
    "    \"\"\"\n",
    "    Chooses an action using the Epsilon-Greedy strategy.\n",
    "    \"\"\"\n",
    "    row, col = state\n",
    "    \n",
    "    # Epsilon-Greedy decision\n",
    "    if random.uniform(0, 1) < current_epsilon:\n",
    "        # Explore: pick a random action\n",
    "        return random.choice(ACTIONS)\n",
    "    else:\n",
    "        # Exploit: pick the best action from the Q-table\n",
    "        # np.argmax finds the index (0, 1, 2, or 3) of the highest Q-value\n",
    "        return np.argmax(Q_table[row, col])\n",
    "\n",
    "def take_action(state, action_index):\n",
    "    \"\"\"\n",
    "    Takes an action, calculates the new state, reward, and if the episode is done.\n",
    "    \"\"\"\n",
    "    current_row, current_col = state\n",
    "    action_row, action_col = ACTION_VECTORS[action_index]\n",
    "    \n",
    "    # Calculate new potential position\n",
    "    new_row = current_row + action_row\n",
    "    new_col = current_col + action_col\n",
    "    \n",
    "    # --- Check for wall collisions ---\n",
    "    # Clamp the row to be within [0, GRID_ROWS - 1]\n",
    "    new_row = max(0, min(new_row, GRID_ROWS - 1))\n",
    "    # Clamp the col to be within [0, GRID_COLS - 1]\n",
    "    new_col = max(0, min(new_col, GRID_COLS - 1))\n",
    "    \n",
    "    new_state = (new_row, new_col)\n",
    "    \n",
    "    # --- Get reward and check if done ---\n",
    "    if new_state == GOAL_STATE:\n",
    "        reward = REWARD_GOAL\n",
    "        done = True\n",
    "    elif new_state == TRAP_STATE:\n",
    "        reward = REWARD_TRAP\n",
    "        done = True\n",
    "    else:\n",
    "        reward = REWARD_STEP\n",
    "        done = False\n",
    "        \n",
    "    return new_state, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Training Loop\n",
    "\n",
    "This is where it all comes together! We will simulate many \"episodes\" (games). In each episode, the agent moves step-by-step until it reaches the Goal or the Trap.\n",
    "\n",
    "At *every single step*, we will:\n",
    "1.  Choose an action.\n",
    "2.  Take the action and get the `(new_state, reward, done)` result.\n",
    "3.  **Update our Q-table** using the Bellman equation.\n",
    "4.  Move to the new state.\n",
    "\n",
    "We will also \"decay\" epsilon over time, so the agent explores a lot at the beginning and then starts exploiting its knowledge more as it learns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Episode 1000/10000 | Epsilon: 0.3746\n",
      "Episode 2000/10000 | Epsilon: 0.1441\n",
      "Episode 3000/10000 | Epsilon: 0.0593\n",
      "Episode 4000/10000 | Epsilon: 0.0282\n",
      "Episode 5000/10000 | Epsilon: 0.0167\n",
      "Episode 6000/10000 | Epsilon: 0.0125\n",
      "Episode 7000/10000 | Epsilon: 0.0109\n",
      "Episode 8000/10000 | Epsilon: 0.0103\n",
      "Episode 9000/10000 | Epsilon: 0.0101\n",
      "Episode 10000/10000 | Epsilon: 0.0100\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "# To store rewards for plotting later\n",
    "episode_rewards = []\n",
    "current_epsilon = epsilon\n",
    "\n",
    "for episode in range(total_episodes):\n",
    "    state = START_STATE\n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # 1. Choose an action\n",
    "        action_index = choose_action(state, current_epsilon)\n",
    "        \n",
    "        # 2. Take the action\n",
    "        new_state, reward, done = take_action(state, action_index)\n",
    "        \n",
    "        # 3. Update the Q-table (The Q-Learning formula)\n",
    "        row, col = state\n",
    "        new_row, new_col = new_state\n",
    "        \n",
    "        old_q_value = Q_table[row, col, action_index]\n",
    "        \n",
    "        # This is max(Q(s', a')) from the formula\n",
    "        best_future_q = np.max(Q_table[new_row, new_col])\n",
    "        \n",
    "        # The core Q-Learning update rule\n",
    "        new_q_value = old_q_value + learning_rate * (reward + discount_factor * best_future_q - old_q_value)\n",
    "        Q_table[row, col, action_index] = new_q_value\n",
    "        \n",
    "        # 4. Update state and reward\n",
    "        state = new_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            break # Episode finished\n",
    "            \n",
    "    # After the episode, store rewards and decay epsilon\n",
    "    episode_rewards.append(total_reward)\n",
    "    \n",
    "    # Decay epsilon\n",
    "    current_epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-epsilon_decay * episode)\n",
    "    \n",
    "    if (episode + 1) % 1000 == 0:\n",
    "        print(f\"Episode {episode + 1}/{total_episodes} | Epsilon: {current_epsilon:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Q-Table:\n",
      " [[[  4.845851     5.49539      4.845851     5.49539   ]\n",
      "  [  5.49031183   6.2171       4.84450843   6.21077327]\n",
      "  [  5.89263163   7.01868123   5.14562941   6.78771638]\n",
      "  [  6.59440562   7.90652295   5.73506318   6.62690522]]\n",
      "\n",
      " [[  4.845851     6.2171       5.49539      6.2171    ]\n",
      "  [  5.49539      7.019        5.49539      7.019     ]\n",
      "  [  6.18609311  -9.99996013   6.21225183   7.91      ]\n",
      "  [  6.98535067   8.9          6.97935395   7.90147564]]\n",
      "\n",
      " [[  5.49321698   7.019        6.21533051   7.01618581]\n",
      "  [  6.2171       7.91         6.2171     -10.        ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [  7.79401761  10.          -9.9820299    8.88755425]]\n",
      "\n",
      " [[  6.21474144   7.01750571   7.01834937   7.91      ]\n",
      "  [  7.019        7.91         7.019        8.9       ]\n",
      "  [-10.           8.89999999   7.91        10.        ]\n",
      "  [  0.           0.           0.           0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Final Q-Table:\\n\", Q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can interpret easily the content of the final Q-table as a **Policy**. For each grid cell, we'll look at its Q-values and find the **action with the highest Q-value**. We'll print an arrow (`‚Üë`, `‚Üì`, `‚Üê`, `‚Üí`) for that action. This shows us the *optimal path* the agent learned from *any* square!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéì Learned Policy (Best action from each state):\n",
      "‚Üì\t‚Üì\t‚Üì\t‚Üì\n",
      "‚Üí\t‚Üì\t‚Üí\t‚Üì\n",
      "‚Üì\t‚Üì\tüî•\t‚Üì\n",
      "‚Üí\t‚Üí\t‚Üí\tüèÜ\n"
     ]
    }
   ],
   "source": [
    "# In[8]:\n",
    "print(\"üéì Learned Policy (Best action from each state):\")\n",
    "\n",
    "# Create a grid to store our policy arrows\n",
    "policy_grid = [[\"\" for _ in range(GRID_COLS)] for _ in range(GRID_ROWS)]\n",
    "\n",
    "for r in range(GRID_ROWS):\n",
    "    for c in range(GRID_COLS):\n",
    "        state = (r, c)\n",
    "        \n",
    "        if state == GOAL_STATE:\n",
    "            policy_grid[r][c] = \"üèÜ\" # Goal\n",
    "        elif state == TRAP_STATE:\n",
    "            policy_grid[r][c] = \"üî•\" # Trap\n",
    "        else:\n",
    "            # Find the best action (index) from this state\n",
    "            best_action_index = np.argmax(Q_table[r, c])\n",
    "            # Map that index to its arrow\n",
    "            policy_grid[r][c] = ACTION_NAMES[best_action_index]\n",
    "\n",
    "# Print the policy grid\n",
    "for row in policy_grid:\n",
    "    # .join(row) combines all elements in the list into a string\n",
    "    # We use \\t (a tab) to space them out nicely\n",
    "    print(\"\\t\".join(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If the training was successful, you should see a \"field\" of arrows all pointing towards the Goal (üèÜ) and steering clear of the Trap (üî•).**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üé¨ Watch the Agent Play\n",
    "\n",
    "Now let's use our learned policy (the Q-table) to play one game *without any exploration* (`epsilon = 0`). We'll see the optimal path it learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 6 | Total Reward: 9.5\n",
      ".\t.\t.\t.\n",
      ".\t.\t.\t.\n",
      ".\t.\tüî•\t.\n",
      ".\t.\t.\tü§ñ\n",
      "\n",
      "üéâ Agent reached the goal! üéâ\n"
     ]
    }
   ],
   "source": [
    "state = START_STATE\n",
    "total_reward = 0\n",
    "step_count = 0\n",
    "\n",
    "for _ in range(max_steps):\n",
    "    # Print the current grid\n",
    "    clear_output(wait=True) # Clears the output for a nice animation\n",
    "    \n",
    "    # Create a temporary grid to print\n",
    "    print_grid = [[\".\" for _ in range(GRID_COLS)] for _ in range(GRID_ROWS)]\n",
    "    print_grid[GOAL_STATE[0]][GOAL_STATE[1]] = \"üèÜ\"\n",
    "    print_grid[TRAP_STATE[0]][TRAP_STATE[1]] = \"üî•\"\n",
    "    print_grid[state[0]][state[1]] = \"ü§ñ\" # Agent's current position\n",
    "    \n",
    "    print(f\"Step: {step_count} | Total Reward: {total_reward:.1f}\")\n",
    "    for row in print_grid:\n",
    "        print(\"\\t\".join(row))\n",
    "\n",
    "    # --- Take the BEST action (no exploration) ---\n",
    "    row, col = state\n",
    "    action_index = np.argmax(Q_table[row, col])\n",
    "    \n",
    "    new_state, reward, done = take_action(state, action_index)\n",
    "    \n",
    "    state = new_state\n",
    "    total_reward += reward\n",
    "    step_count += 1\n",
    "    \n",
    "    time.sleep(0.5) # Pause for 0.5 seconds to see the move\n",
    "    \n",
    "    if done:\n",
    "        # Print the final state\n",
    "        clear_output(wait=True)\n",
    "        print_grid = [[\".\" for _ in range(GRID_COLS)] for _ in range(GRID_ROWS)]\n",
    "        print_grid[GOAL_STATE[0]][GOAL_STATE[1]] = \"üèÜ\"\n",
    "        print_grid[TRAP_STATE[0]][TRAP_STATE[1]] = \"üî•\"\n",
    "        print_grid[state[0]][state[1]] = \"ü§ñ\"\n",
    "        \n",
    "        print(f\"Step: {step_count} | Total Reward: {total_reward:.1f}\")\n",
    "        for row in print_grid:\n",
    "            print(\"\\t\".join(row))\n",
    "        \n",
    "        if state == GOAL_STATE:\n",
    "            print(\"\\nüéâ Agent reached the goal! üéâ\")\n",
    "        else:\n",
    "            print(\"\\n‚ò†Ô∏è Agent fell in the trap! ‚ò†Ô∏è\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wip-clase (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
