{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "dCH3qvjHR_A6"
   },
   "source": [
    "# *Clustering* with K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a modified version of [Professor Cristina Gómez's more detailed notebook](https://github.com/cristinagom/machinelearning/blob/main/ejercicios/UD3-UnsupervisedLearning/ML_3.1_UnsupervisedLearning_Clustering/ML_3_1_UnsupervisedLearning_Clustering.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "KLJG4vwlR_BC"
   },
   "source": [
    "***Clustering*** is an **unsupervised learning** technique that groups records into groups of similar characteristics, known as ***clusters***.\n",
    "\n",
    "**K-means** is a clustering algorithm that groups data by attempting to separate samples into $K$ predefined groups, where each sample belongs to the group whose mean (the cluster **centroid**) is closest.\n",
    "\n",
    "The groups are established by minimizing the **inertia or WCSS (Within-Cluster Sum of Squares)**, which is the sum of squared distances from each sample to the centroid of its group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "MOhAE83TR_BE",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The K-means algorithm follows these steps:\n",
    "1. **Initialization**: Choose $K$ initial centroids randomly or using some specific method.\n",
    "2. **Assignment**: Assign each sample to the nearest centroid.\n",
    "3. **Update**: Recalculate the centroids as the mean of the samples in each group.\n",
    "4. **Iteration**: Repeat steps 2 and 3 until the centroids no longer change (the algorithm converges) or a maximum number of iterations is reached.\n",
    "\n",
    "[![K-means](img/K-means_convergence.gif)](https://upload.wikimedia.org/wikipedia/commons/e/ea/K-means_convergence.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `make_blobs` function from `sklearn.datasets` to generate test data. This function generates a dataset of points distributed as point clouds. We pass the following parameters:\n",
    "- `n_samples`: total number of samples\n",
    "- `n_features`: number of features for each sample. We will work with only 2 features to be able to visualize the data in 2D.\n",
    "- `centers`: number of clusters we want to generate\n",
    "- `random_state`: seed for random number generation\n",
    "- `cluster_std`: standard deviation of the clusters\n",
    "\n",
    "It's important to remember that we are doing this representation process to help understand the algorithm, but we will normally work with datasets of many more dimensions where we cannot visualize the data. In fact, in this case it would normally be easy to divide into clusters by eye. And indeed, we are already generating test data with this `make_blobs` function by indicating the number of clusters we want it to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6vDuxc5bR_BJ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "X, y = datasets.make_blobs(n_samples=1000, # Number of samples\n",
    "                           n_features=2, # Number of features\n",
    "                           centers=5, # Number of clusters\n",
    "                           cluster_std=[0.5, 0.6, 0.8, 1, 1.1], # Standard deviation of each cluster\n",
    "                           random_state=42,) # Seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], s=2)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0AI8VEdFR_BM",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(\n",
    "    n_clusters=5, # Number of clusters (k value)\n",
    "    n_init='auto') # Number of times the algorithm runs with different initial centroids\n",
    "y_pred = kmeans.fit_predict(X) # Equivalent to kmeans.fit(X) and then y_pred = kmeans.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KMeans constructor receives the following parameters:\n",
    "* `n_clusters`: The number of clusters to form and the number of centroids to generate. This is what we vary in a loop to find the optimal number of clusters.\n",
    "* `init`: Method for initializing centroids. The simplest is 'random', but by default it uses 'k-means++', which is a more sophisticated method that tries to place initial centroids far apart from each other.\n",
    "* `max_iter`: Maximum number of iterations of the algorithm for a single run.\n",
    "* `n_init`: Number of times the algorithm will be run with different centroids. The final result will be the best output of n_init consecutive runs in terms of inertia.\n",
    "\n",
    "Now, with the WCSS array, we can use the elbow method to determine the optimal number of clusters. The elbow method is based on the variation of inertia (WCSS) as a function of the number of clusters. If we increase the number of clusters, inertia will decrease. The goal is to choose a number of clusters where inertia decreases significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "FgNN5i2uR_BP"
   },
   "source": [
    "Like other `scikit-learn` estimators, KMeans has a `fit_predict` method that can be used to train the model and predict the clusters to which the data belongs. This method returns an array of cluster labels, which assigns each data point to a cluster, and also stores it in the `labels_` property of the KMeans object.\n",
    "\n",
    "With the `is` operator we can check if two variables point to the same memory address, that is, if they are the same object. In this case, we are checking if the `labels_` attribute of the KMeans object is the same as the label array returned by the `fit_predict` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 206,
     "status": "ok",
     "timestamp": 1679501313278,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -60
    },
    "id": "qY3_ue8pR_BP",
    "outputId": "f93be59c-adba-4599-c7b6-68c03e9432ab",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_pred is kmeans.labels_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "VJNI6-PQR_BQ"
   },
   "source": [
    "The cluster centroids can be obtained with the `cluster_centers_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "ZOA4w8o2vAMK",
    "outputId": "c92c5dae-d037-4971-e7e7-e515c89a4660"
   },
   "outputs": [],
   "source": [
    "print(kmeans.cluster_centers_)\n",
    "plt.scatter(X[:, 0], X[:, 1], s=2)\n",
    "plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], c='r', s=30)\n",
    "\n",
    "for i, center in enumerate(kmeans.cluster_centers_):\n",
    "    plt.annotate(i, center, fontsize=18)\n",
    "\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "fj3aiHzlR_BR"
   },
   "source": [
    "We can easily predict the cluster to which each new instance belongs with the `predict` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yZTdB0CZR_BR",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, -2.5], [-3, 5]])\n",
    "y_new = kmeans.predict(X_new)\n",
    "y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 274,
     "status": "ok",
     "timestamp": 1679501496174,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -60
    },
    "id": "Ffb5-jmiR_BS",
    "outputId": "7a7e8d5b-25b7-4863-c61e-52f8dbb4d027",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(kmeans.cluster_centers_)\n",
    "plt.scatter(X[:, 0], X[:, 1], s=2)\n",
    "plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], c='r', s=20)\n",
    "plt.scatter(X_new[:, 0], X_new[:, 1], c='g', s=50)\n",
    "\n",
    "for i, center in enumerate(kmeans.cluster_centers_):\n",
    "    plt.annotate(i, center, fontsize=18)\n",
    "    \n",
    "for i, point in enumerate(X_new): # Annotation for new points\n",
    "    plt.annotate(f'x{i}={y_new[i]}', point, fontsize=12)\n",
    "\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "uoL15F9pR_BS"
   },
   "source": [
    "By plotting the decision boundaries of the algorithm, we obtain a [Voronoi tessellation](https://en.wikipedia.org/wiki/Voronoi_diagram):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qlyup9-lR_BT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_decision_regions(kmeans, X):\n",
    "  x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 # Min and max values for x1\n",
    "  y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 # Min and max values for x2\n",
    "  xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), \n",
    "                      np.arange(y_min, y_max, 0.1)) \n",
    "  \n",
    "  Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "  Z = Z.reshape(xx.shape)\n",
    "\n",
    "  plt.figure(figsize=(8, 8))\n",
    "  plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "  plt.scatter(X[:, 0], X[:, 1], c=y_pred, s=20, edgecolor='k')\n",
    "  plt.show()\n",
    "  \n",
    "plot_decision_regions(kmeans,X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "1mcyvKUCR_BU"
   },
   "source": [
    "The vast majority of instances were clearly assigned to their original cluster.\n",
    "\n",
    "The only thing K-means cares about is the distance between instances and centroids. Instead of assigning each instance to a cluster (hard clustering), it's better to give a cluster score per instance (soft clustering). The score can be the distance between the instance and the centroids.\n",
    "\n",
    "The `transform` method measures the distance between each instance and the centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 207,
     "status": "ok",
     "timestamp": 1679501550513,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -60
    },
    "id": "COOy5MDSR_BV",
    "outputId": "4adbe6ea-8677-48f8-c6cc-e2ac2b4eaedc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "kmeans.transform(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "YCEdrcK9R_BX"
   },
   "source": [
    "## Choosing the Number of Clusters (*Elbow Method*)\n",
    "\n",
    "We might think we can choose the model with the lowest inertia. This poses a problem because increasing `k` will always give us lower inertia (or distortion).\n",
    "\n",
    "The elbow method is a heuristic used to find the optimal number of clusters in a clustering algorithm. The idea is to run it in a loop calculating inertia and choose the value of k where inertia stops decreasing rapidly.\n",
    "\n",
    "Let's visualize inertia as a function of `k`:\n",
    "\n",
    "<img src=\"img/k_to_inertia.png\" width=\"700\" />\n",
    "\n",
    "As we can see, distortion (or inertia) drops significantly when going from $3$ to $4$, but then decreases much more slowly as we continue increasing $k$. This curve has roughly the shape of an arm with an elbow at $k=4$.\n",
    "\n",
    "However, it should be noted that this method is somewhat subjective and depends on the shape of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6t-RnW28R_BX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inertia = []\n",
    "K = range(1,10)\n",
    "for k in K:\n",
    "    model = KMeans(k).fit(X)\n",
    "    inertia.append(model.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "executionInfo": {
     "elapsed": 677,
     "status": "ok",
     "timestamp": 1679501868812,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -60
    },
    "id": "WCLW74P1R_BY",
    "outputId": "886c0e2d-a47a-48f4-ac0f-ae184c9a16da",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(K, inertia, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Finding the Optimal K using the Elbow Method')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "ztAhvA80R_Bc"
   },
   "source": [
    "## Limitations of K-means\n",
    "\n",
    "- `K-means` is not perfect, so it is necessary to run the algorithm several times to avoid suboptimal solutions.\n",
    "- Another limiting factor of the algorithm is that we need to specify the number of clusters.\n",
    "- `K-means` also doesn't perform very well when groups have different sizes, different densities, or non-spherical shapes.\n",
    "- Depending on the data, different clustering algorithms may work better (such as `DBSCAN` or `Gaussian Mixtures`).\n",
    "- Scaling the inputs with a StandardScaler is also essential with `K-means`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/clustering.html#k-means\n",
    "- https://www.datacamp.com/tutorial/k-means-clustering-python\n",
    "- [StatQuest: K-means clustering](https://www.youtube.com/watch?v=4b5d3muPQmA)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "wip-clase (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
